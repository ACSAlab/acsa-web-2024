---
title: "A review of deep learning-based medical image segmentation methods"
FirstAuthor:
- Jun Shi
- Tiantong Wang
OtherAuthors:
- Ziqi Zhu
- Minfan Zhao
- Bingxun Wang
- Junshi Chen
- Hong An
ConfJournal: "中国图像图形学报"
ConfJournalAbbr:
IsAConference: "no" # 会议填yes，期刊写 no
CCFLevel: "C" 
Year: 2025
Award: ""
Abstract: ""
KeyWords:
- deep learning
- medical image segmentation
- convolutional neural network 
- vision transformer 
- vision mamba
Link: http://cjig.ijournals.cn/jig/ch/reader/view_abstract.aspx?flag=2&file_no=202408070000004&journal_id=jig # 官网链接 
PDF: # pdf文件位置
SLIDE:  # PPT文件位置
video: # 会议视频
---

Medical image segmentation is a crucial component of clinical medical image analysis, aimed at accurately identifying and delineating anatomical structures or regions of interest, such as lesions, within medical images. This provides objective and quantitative support for decision-making in disease diagnosis, treatment planning, and postoperative evaluation. In recent years, the rapid growth of available annotated data has facilitated the swift development of deep learning-based medical image segmentation methods, which demonstrate superior accuracy and robustness compared to traditional segmentation techniques, thereby becoming the mainstream technology in the field. To further enhance segmentation accuracy, extensive research has focused on improving the structural designs of segmentation models, resulting in a variety of distinct segmentation approaches. Current deep learning-based medical image segmentation methods can be classified into three main structural categories: Convolutional Neural Networks (CNNs), Vision Transformers, and Vision Mamba. As a representative neural network architecture, CNNs effectively capture spatial features in images through their unique local receptive fields and weight-sharing mechanisms, making them particularly suitable for image analysis and processing tasks. Since 2015, CNN-based methods, exemplified by U-Net, have dominated the field of medical image segmentation, consistently achieving state-of-the-art performance across various downstream segmentation tasks. To further improve segmentation accuracy, many studies have focused on modifying and innovating the U-Net structure, leading to a series of derived segmentation methods. However, the inherent limitations of convolutional operators, particularly their local receptive fields, restrict these methods' ability to capture global contextual dependencies, especially when handling complex medical images and fine-grained segmentation targets. While techniques such as attention mechanisms and specialized convolutions have somewhat alleviated this issue and enhanced the model's focus on global information, their effectiveness remains limited. Since 2020, researchers have begun to introduce Transformer architectures, originally developed in the natural language processing (NLP) domain, into computer vision tasks, including medical image segmentation. Vision Transformers utilize self-attention mechanisms to effectively model global dependencies, significantly improving the quality of semantic feature extraction and facilitating the segmentation of complex medical images. Transformer-based methods for medical image segmentation mainly include hybrid approaches that combine Transformers with CNNs and pure Transformer methods, each showcasing unique advantages and disadvantages. Hybrid approaches leverage CNNs' strengths in local feature extraction alongside Transformers' capabilities in modeling global context, thereby enhancing segmentation accuracy while maintaining computational efficiency. However, these methods remain dependent on CNN structures, which may limit their performance in complex scenarios. In contrast, pure Transformer methods excel in capturing long-range dependencies and multiscale features, significantly improving segmentation accuracy and generalization. Nevertheless, pure Transformer architecture typically requires substantial computational resources and high-quality training data, posing challenges in obtaining large-scale annotated datasets in the medical field. Despite the notable advantages of Transformer structures in capturing long-range dependencies and global contextual information, their computational complexity grows quadratically with the length of the input sequence, limiting their applicability in resource-constrained environments. To overcome this challenge, researchers are developing new methods capable of modeling global dependencies with linear time complexity. Mamba introduces a novel selective state-space model that employs a selection mechanism, hardware-aware algorithms, and a simpler architecture to reduce computational complexity while maintaining efficient long-sequence modeling performance significantly. Consequently, since 2024, numerous studies have begun to apply the Mamba structure to medical image segmentation tasks, achieving promising results and potentially replacing Transformer structures. The hybrid method combining Mamba with CNNs can more effectively enhance segmentation accuracy and robustness by integrating CNN's feature extraction capabilities with Mamba's handling of long-range dependencies. However, this approach may increase computational complexity during integration. Additionally, pure Mamba methods are more suitable for segmentation tasks requiring global contextual information but still face limitations in capturing spatial features of images and may demand greater computational resources during training. In summary, this paper systematically reviews and analyzes the development trajectory, advantages, and limitations of deep learning-based medical image segmentation methods from a structural perspective for the first time. First, we categorize all surveyed methods into three structural classes. We then provide a brief overview of the structural evolution of different segmentation methods, analyzing their structural characteristics, strengths, and weaknesses. Subsequently, we delve into the major challenges and opportunities currently facing the field of medical image segmentation from multiple perspectives, including algorithm structure, learning methods, and task paradigms. Finally, we conduct an in-depth analysis and discussion of future development directions and application prospects.
